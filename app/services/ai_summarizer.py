"""AI æ€»ç»“æ¨¡å— (Gemini + DeepSeek + Sonnet)"""
import base64
import os
import logging
import httpx
from app.config import (
    API_BASE_URL,
    GEMINI_API_KEY, GEMINI_MODEL,
    DEEPSEEK_API_KEY, DEEPSEEK_MODEL,
    SONNET_API_KEY, SONNET_MODEL,
)

logger = logging.getLogger(__name__)


async def _chat(model, messages, api_key, max_tokens=8192, temperature=0.3, timeout=180) -> str:
    """OpenAI å…¼å®¹å¯¹è¯æ¥å£"""
    url = f"{API_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
    }

    async with httpx.AsyncClient(timeout=timeout) as client:
        resp = await client.post(url, headers=headers, json=payload)
        resp.raise_for_status()
        return resp.json()["choices"][0]["message"]["content"]


# ======================== å…¨å±€ Prompt ========================

STAGE1_SYSTEM = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹è½¬å†™ä¸æ€»ç»“åŠ©æ‰‹ã€‚

è¯·å®Œæˆä¸¤ä»¶äº‹ï¼š
1. å®Œæ•´è½¬å†™éŸ³é¢‘ä¸­çš„æ‰€æœ‰å£è¿°å†…å®¹ï¼ˆä¸è¦é—æ¼ä»»ä½•è§‚ç‚¹ã€æ•°æ®ã€æ¡ˆä¾‹ï¼‰
2. åŸºäºè½¬å†™å†…å®¹ï¼Œè¾“å‡ºä¸€ä»½ç»“æ„åŒ–çš„ Markdown å­¦ä¹ ç¬”è®°

## è¾“å‡ºæ ¼å¼
# è§†é¢‘æ ‡é¢˜
> æ ¸å¿ƒæ‘˜è¦ï¼šä¸€å¥è¯æ¦‚æ‹¬
## æ ¸å¿ƒè¦ç‚¹
1. **è¦ç‚¹ä¸€**ï¼šè¯´æ˜
...
## è¯¦ç»†ç¬”è®°
### å°èŠ‚æ ‡é¢˜
- å…·ä½“å†…å®¹...
## å…³é”®æ”¶è·
1. ...
## åŸå§‹è½¬å†™æ–‡æœ¬
> åœ¨æ­¤å¤„æ”¾ç½®å®Œæ•´çš„é€å­—è½¬å†™å†…å®¹ï¼Œç”¨å¼•ç”¨å—åŒ…è£¹ã€‚
"""

STAGE2_SYSTEM = """ä½ æ˜¯ä¸€ä½åšå­¦ä¸¥è°¨çš„çŸ¥è¯†å®¡è®¡ä¸“å®¶ã€‚è¯·å¯¹ AI ç”Ÿæˆçš„å­¦ä¹ ç¬”è®°åˆç¨¿è¿›è¡Œæ·±åº¦å®¡è§†ã€‚

ä»»åŠ¡ï¼š
1. å†…å®¹ç¼ºå¤±å®¡æŸ¥ï¼šæ£€æŸ¥æœªå®šä¹‰çš„æœ¯è¯­ã€æœªä»‹ç»çš„äººç‰©/èƒŒæ™¯ã€‚
2. æ·±åº¦ä¸è¶³è¯Šæ–­ï¼šæŒ‡å‡ºç¼ºä¹è®ºè¯çš„è§‚ç‚¹ã€‚
3. çŸ¥è¯†æ‹“å±•å»ºè®®ï¼šè¡¥å……å…³è”çŸ¥è¯†å’Œå»¶ä¼¸é˜…è¯»ã€‚

## è¾“å‡ºæ ¼å¼ (Markdown)
# å®¡æŸ¥æŠ¥å‘Š
## éœ€è¦è¡¥å……è§£é‡Šçš„æ¦‚å¿µ
1. **[æ¦‚å¿µ]** â€” ç†ç”± + æœç´¢å…³é”®è¯
## éœ€è¦è¡¥å……çš„èƒŒæ™¯ä¿¡æ¯
...
## å»ºè®®è¡¥å……çš„å…³è”çŸ¥è¯†
...
## å…·ä½“æœç´¢ä»»åŠ¡æ¸…å•
1. æœç´¢: "[å…³é”®è¯]" â€” ç”¨äºè¡¥å…… [å†…å®¹]
...
"""

STAGE3_SYSTEM = """ä½ æ˜¯ä¸€ä½é¡¶çº§çŸ¥è¯†ç¼–è¾‘ã€‚è¯·å°†åˆç¨¿é‡å†™ä¸ºä¸€ä»½å®Œæ•´ã€æ·±å…¥ã€æ ·å¼ç²¾ç¾çš„æœ€ç»ˆç‰ˆç¬”è®°ã€‚

## æ ¸å¿ƒåŸåˆ™
1. **ç»“æ„ç¬¬ä¸€**ï¼šç›´æ¥è¾“å‡ºç¬”è®°ï¼Œæ— åºŸè¯ã€‚
2. **æ ·å¼è§„èŒƒ**ï¼š
   - ä¸¥ç¦æ­£æ–‡ä½¿ç”¨å¼•ç”¨å—ã€‚
   - æ•°å­¦å…¬å¼ï¼šè¡Œå†… $...$ (ä¸­æ–‡ç¯å¢ƒç¦æ­¢ LaTeX)ï¼Œå—çº§ $$...$$ã€‚
3. **å†…å®¹æ·±åº¦**ï¼šè§£é‡Šä¸“ä¸šåè¯ï¼Œè¡¥å……èƒŒæ™¯ã€‚

## è¾“å‡ºç»“æ„
# [æ ‡é¢˜]
> **æ ¸å¿ƒæ‘˜è¦**ï¼š...
> **è§†é¢‘ä½œè€…**ï¼š...
## 1. [å°èŠ‚]
...
## å»¶ä¼¸é˜…è¯»
...
"""


# ======================== Stage 1: Gemini ========================

async def stage1_transcribe_and_draft(audio_path, video_title="", video_author="", user_requirement="") -> str:
    """Gemini å¤šæ¨¡æ€: éŸ³é¢‘ â†’ åˆç¨¿"""
    logger.info("[Stage1] Gemini è½¬å†™+åˆç¨¿")

    if os.path.getsize(audio_path) > 24 * 1024 * 1024:
        # å¤§æ–‡ä»¶å›é€€å¤„ç†
        return await _stage1_large_audio(audio_path, video_title, video_author, user_requirement)

    with open(audio_path, "rb") as f:
        audio_b64 = base64.b64encode(f.read()).decode()

    user_parts = _build_context(video_title, video_author, user_requirement)
    messages = [
        {"role": "system", "content": STAGE1_SYSTEM},
        {
            "role": "user",
            "content": [
                {"type": "input_audio", "input_audio": {"data": audio_b64, "format": "mp3"}},
                {"type": "text", "text": user_parts},
            ],
        },
    ]

    try:
        return await _chat(GEMINI_MODEL, messages, GEMINI_API_KEY, timeout=240)
    except Exception as e:
        logger.warning(f"[Stage1] å¤±è´¥ï¼Œå›é€€: {e}")
        return await _stage1_fallback(audio_path, video_title, video_author, user_requirement)


async def _stage1_fallback(audio_path, title, author, req) -> str:
    """WHISPER è½¬å†™ + LLM æ€»ç»“"""
    transcript = await _transcribe_audio(audio_path)
    prompt = f"{_build_context(title, author, req)}\n\nè½¬å†™æ–‡æœ¬:\n\n{transcript}"
    messages = [
        {"role": "system", "content": STAGE1_SYSTEM},
        {"role": "user", "content": prompt},
    ]
    return await _chat(GEMINI_MODEL, messages, GEMINI_API_KEY)


async def _stage1_large_audio(audio_path, title, author, req) -> str:
    """å¤§æ–‡ä»¶åˆ†æ®µè½¬å†™"""
    # çœç•¥å…·ä½“å®ç°ç»†èŠ‚ï¼Œä¿æŒåŸæœ‰é€»è¾‘ä½†ç®€åŒ–ä»£ç ç»“æ„
    # è¿™é‡Œä¸ºäº†ä¿æŒåŠŸèƒ½å®Œæ•´æ€§ï¼Œä¿ç•™æ ¸å¿ƒé€»è¾‘ä½†ç®€åŒ–æ³¨é‡Š
    import subprocess
    probe = subprocess.run(["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", audio_path], capture_output=True, text=True)
    duration = float(probe.stdout.strip())

    segments, start = [], 0
    while start < duration:
        seg = audio_path.replace(".mp3", f"_seg{int(start)}.mp3")
        subprocess.run(["ffmpeg", "-ss", str(start), "-i", audio_path, "-t", "600", "-acodec", "libmp3lame", "-y", seg], capture_output=True)
        if os.path.exists(seg): segments.append(seg)
        start += 600

    parts = []
    for seg in segments:
        try: parts.append(await _transcribe_audio(seg))
        except: pass
        finally: 
            if os.path.exists(seg): os.remove(seg)

    transcript = "\n".join(parts)
    prompt = f"{_build_context(title, author, req)}\n\nè½¬å†™æ–‡æœ¬:\n\n{transcript}"
    return await _chat(GEMINI_MODEL, [{"role": "system", "content": STAGE1_SYSTEM}, {"role": "user", "content": prompt}], GEMINI_API_KEY)


async def _transcribe_audio(audio_path: str) -> str:
    """Whisper API è½¬å†™"""
    url = f"{API_BASE_URL}/audio/transcriptions"
    headers = {"Authorization": f"Bearer {GEMINI_API_KEY}"}
    try:
        async with httpx.AsyncClient(timeout=180) as client:
            with open(audio_path, "rb") as f:
                resp = await client.post(url, headers=headers, files={"file": (os.path.basename(audio_path), f, "audio/mpeg")}, data={"model": "whisper-1", "language": "zh"})
                resp.raise_for_status()
                return resp.text
    except Exception:
        # Fallback to Gemini Multimodal
        with open(audio_path, "rb") as f: b64 = base64.b64encode(f.read()).decode()
        return await _chat(GEMINI_MODEL, [{"role": "user", "content": [{"type": "input_audio", "input_audio": {"data": b64, "format": "mp3"}}, {"type": "text", "text": "è½¬å†™ä¸ºä¸­æ–‡æ–‡æœ¬"}]}], GEMINI_API_KEY, temperature=0.1)


# ======================== Stage 2: DeepSeek ========================

async def stage2_critical_review(draft_markdown: str) -> str:
    """DeepSeek æ·±åº¦å®¡è§†"""
    logger.info("[Stage2] DeepSeek æ·±åº¦å®¡è§†")
    messages = [
        {"role": "system", "content": STAGE2_SYSTEM},
        {"role": "user", "content": f"ä»¥ä¸‹æ˜¯åˆç¨¿ï¼Œè¯·å®¡è§†ï¼š\n\n---\n{draft_markdown}\n---\n\nè¯·è¾“å‡ºå®¡æŸ¥æŠ¥å‘Šã€‚"},
    ]
    return await _chat(DEEPSEEK_MODEL, messages, DEEPSEEK_API_KEY, max_tokens=4096, temperature=0.2, timeout=300)


# ======================== Stage 3: Sonnet ========================

async def stage3_enrich_and_finalize(draft_markdown, review_report, user_requirement="") -> str:
    """Sonnet è”ç½‘æœç´¢ + æœ€ç»ˆç‰ˆ"""
    logger.info("[Stage3] Sonnet è”ç½‘æœç´¢")
    user_content = f"## åˆç¨¿\n{draft_markdown}\n\n## å®¡æŸ¥æŠ¥å‘Š\n{review_report}\n"
    if user_requirement: user_content += f"\n## ç”¨æˆ·è¦æ±‚\n{user_requirement}\n"
    user_content += "\nè¯·æ‰§è¡Œæœç´¢ä»»åŠ¡å¹¶è¾“å‡ºæœ€ç»ˆç‰ˆç¬”è®°ã€‚"

    messages = [{"role": "system", "content": STAGE3_SYSTEM}, {"role": "user", "content": user_content}]
    
    # Sonnet å·¥å…·è°ƒç”¨
    url = f"{API_BASE_URL}/chat/completions"
    headers = {"Authorization": f"Bearer {SONNET_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": SONNET_MODEL, "messages": messages, "max_tokens": 12000, "temperature": 0.3,
        "tools": [{"type": "web_search_20250305", "name": "web_search"}]
    }

    async with httpx.AsyncClient(timeout=300) as client:
        resp = await client.post(url, headers=headers, json=payload)
        resp.raise_for_status()
        data = resp.json()

    content = data["choices"][0]["message"].get("content", "")
    if isinstance(content, list):
        return "\n".join(b.get("text", "") for b in content if b.get("type") == "text")
    return content


async def summarize_with_audio(audio_path, video_title="", video_author="", user_requirement="", progress_callback=None) -> str:
    """ä¸‰é˜¶æ®µ AI æ€»ç»“æµæ°´çº¿"""
    async def notify(msg):
        if progress_callback: await progress_callback(msg)

    await notify("ğŸ”¬ [1/3] Gemini è½¬å†™ç”Ÿæˆåˆç¨¿...")
    draft = await stage1_transcribe_and_draft(audio_path, video_title, video_author, user_requirement)
    
    await notify("ğŸ§  [2/3] DeepSeek æ·±åº¦å®¡è§†...")
    review = await stage2_critical_review(draft)
    
    await notify("ğŸŒ [3/3] Sonnet è”ç½‘æœç´¢ç”Ÿæˆç»ˆç¨¿...")
    final = await stage3_enrich_and_finalize(draft, review, user_requirement)
    
    await notify("âœ… å¤„ç†å®Œæˆ")
    return final


def _build_context(title, author, requirement):
    parts = ["è¯·å¯¹ä»¥ä¸‹è§†é¢‘å†…å®¹è¿›è¡Œè½¬å†™å’Œæ€»ç»“ï¼š"]
    if title: parts.append(f"æ ‡é¢˜ï¼š{title}")
    if author: parts.append(f"ä½œè€…ï¼š{author}")
    if requirement: parts.append(f"\nç”¨æˆ·ç‰¹åˆ«è¦æ±‚ï¼š{requirement}")
    return "\n".join(parts)
